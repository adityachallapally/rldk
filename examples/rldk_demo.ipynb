{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLDK Demo Experience\n",
    "\n",
    "This notebook demonstrates the complete RLDK (Reinforcement Learning Debugging Kit) experience, showing how it detects and helps fix real RL training failures through comprehensive analysis.\n",
    "\n",
    "## What is RLDK?\n",
    "\n",
    "RLDK is a comprehensive debugging toolkit for reinforcement learning training. It provides:\n",
    "- **Run Comparison**: Detect when training runs diverge\n",
    "- **Checkpoint Analysis**: Compare model parameters between checkpoints\n",
    "- **Environment Auditing**: Check for determinism issues\n",
    "- **PPO Forensics**: Detect PPO-specific anomalies like KL spikes\n",
    "- **Reward Drift Detection**: Identify when reward models drift apart\n",
    "- **Comprehensive Diagnostics**: All-in-one health check\n",
    "\n",
    "## Demo Overview\n",
    "\n",
    "This demo will:\n",
    "1. Install RLDK\n",
    "2. Generate test artifacts (training logs, checkpoints, prompts)\n",
    "3. Run each RLDK analysis tool\n",
    "4. Visualize results\n",
    "5. Show how RLDK detects real training issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install RLDK in development mode\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate Test Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training logs\n",
    "print(\"üîÑ Generating training logs...\")\n",
    "!python generate_logs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify artifacts were created\n",
    "required_files = [\n",
    "    \"test_artifacts/logs_clean/training.jsonl\",\n",
    "    \"test_artifacts/logs_doctored_kl_spike/training.jsonl\",\n",
    "    \"test_artifacts/reward_drift_demo/prompts.jsonl\"\n",
    "]\n",
    "\n",
    "for file_path in required_files:\n",
    "    if Path(file_path).exists():\n",
    "        print(f\"‚úÖ {file_path}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {file_path} - Missing!\")\n",
    "\n",
    "print(\"\\nüìä Artifact verification complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Visualize Training Data\n",
    "\n",
    "Let's first examine the training logs to understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and visualize training logs\n",
    "def load_training_logs(file_path):\n",
    "    \"\"\"Load training logs from JSONL file.\"\"\"\n",
    "    logs = []\n",
    "    with open(file_path) as f:\n",
    "        for line in f:\n",
    "            logs.append(json.loads(line.strip()))\n",
    "    return pd.DataFrame(logs)\n",
    "\n",
    "# Load both log files\n",
    "clean_logs = load_training_logs(\"test_artifacts/logs_clean/training.jsonl\")\n",
    "doctored_logs = load_training_logs(\"test_artifacts/logs_doctored_kl_spike/training.jsonl\")\n",
    "\n",
    "print(f\"Clean logs: {len(clean_logs)} steps\")\n",
    "print(f\"Doctored logs: {len(doctored_logs)} steps\")\n",
    "print(\"\\nClean logs preview:\")\n",
    "print(clean_logs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot KL divergence comparison\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Clean logs\n",
    "ax1.plot(clean_logs['step'], clean_logs['kl'], label='KL Divergence', color='blue')\n",
    "ax1.set_title('Clean Training Logs - KL Divergence')\n",
    "ax1.set_ylabel('KL Divergence')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# Doctored logs\n",
    "ax2.plot(doctored_logs['step'], doctored_logs['kl'], label='KL Divergence', color='red')\n",
    "ax2.axvline(x=800, color='orange', linestyle='--', label='KL Spike Start')\n",
    "ax2.set_title('Doctored Training Logs - KL Divergence (Spike at Step 800)')\n",
    "ax2.set_xlabel('Training Step')\n",
    "ax2.set_ylabel('KL Divergence')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìà KL divergence comparison plotted. Notice the spike in doctored logs starting at step 800!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: RLDK Analysis - Run Comparison\n",
    "\n",
    "Now let's use RLDK to detect when the two training runs start to diverge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RLDK compare-runs\n",
    "print(\"üîç Running RLDK compare-runs analysis...\")\n",
    "!rldk compare-runs test_artifacts/logs_clean test_artifacts/logs_doctored_kl_spike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if divergence report was generated\n",
    "if Path(\"rldk_reports/divergence_report.json\").exists():\n",
    "    with open(\"rldk_reports/divergence_report.json\") as f:\n",
    "        divergence_data = json.load(f)\n",
    "\n",
    "    print(\"‚úÖ Divergence report generated!\")\n",
    "    print(f\"First divergence detected at step: {divergence_data.get('first_divergence_step', 'Unknown')}\")\n",
    "    print(f\"Divergence metric: {divergence_data.get('divergence_metric', 'Unknown')}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Divergence report not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: RLDK Analysis - Checkpoint Comparison\n",
    "\n",
    "Let's compare checkpoints to see parameter differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare identical checkpoints (should show no differences)\n",
    "print(\"üîç Comparing identical checkpoints...\")\n",
    "!rldk diff-ckpt test_artifacts/ckpt_identical/a.pt test_artifacts/ckpt_identical/b.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare checkpoints with value head differences\n",
    "print(\"\\nüîç Comparing checkpoints with value head differences...\")\n",
    "!rldk diff-ckpt test_artifacts/ckpt_value_head_edit/a.pt test_artifacts/ckpt_value_head_edit/b.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if checkpoint diff report was generated\n",
    "if Path(\"rldk_reports/ckpt_diff.json\").exists():\n",
    "    with open(\"rldk_reports/ckpt_diff.json\") as f:\n",
    "        ckpt_data = json.load(f)\n",
    "\n",
    "    print(\"‚úÖ Checkpoint diff report generated!\")\n",
    "    print(f\"Total parameter differences: {ckpt_data.get('total_differences', 'Unknown')}\")\n",
    "    print(f\"Max difference magnitude: {ckpt_data.get('max_difference', 'Unknown')}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Checkpoint diff report not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: RLDK Analysis - Environment Audit\n",
    "\n",
    "Let's check for determinism issues in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run environment audit\n",
    "print(\"üîç Running environment audit for determinism...\")\n",
    "!rldk env-audit test_artifacts/logs_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if determinism report was generated\n",
    "if Path(\"rldk_reports/determinism_card.json\").exists():\n",
    "    with open(\"rldk_reports/determinism_card.json\") as f:\n",
    "        det_data = json.load(f)\n",
    "\n",
    "    print(\"‚úÖ Environment audit completed!\")\n",
    "    print(f\"Determinism score: {det_data.get('determinism_score', 'Unknown')}\")\n",
    "    print(f\"Issues found: {det_data.get('issues_count', 'Unknown')}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Determinism report not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: RLDK Analysis - PPO Log Scanning\n",
    "\n",
    "Let's scan for PPO-specific anomalies like KL spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan clean logs for anomalies\n",
    "print(\"üîç Scanning clean logs for PPO anomalies...\")\n",
    "!rldk log-scan test_artifacts/logs_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan doctored logs for KL spike detection\n",
    "print(\"\\nüîç Scanning doctored logs for KL spike detection...\")\n",
    "!rldk log-scan test_artifacts/logs_doctored_kl_spike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if PPO scan report was generated\n",
    "if Path(\"rldk_reports/ppo_scan.json\").exists():\n",
    "    with open(\"rldk_reports/ppo_scan.json\") as f:\n",
    "        ppo_data = json.load(f)\n",
    "\n",
    "    print(\"‚úÖ PPO scan completed!\")\n",
    "    print(f\"Anomalies detected: {ppo_data.get('anomalies_count', 'Unknown')}\")\n",
    "    print(f\"KL spike detected: {ppo_data.get('kl_spike_detected', 'Unknown')}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è PPO scan report not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: RLDK Analysis - Reward Drift Detection\n",
    "\n",
    "Let's detect if reward models have drifted apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run reward drift detection\n",
    "print(\"üîç Running reward drift detection...\")\n",
    "!rldk reward-drift test_artifacts/reward_drift_demo/rmA test_artifacts/reward_drift_demo/rmB --prompts test_artifacts/reward_drift_demo/prompts.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if reward drift report was generated\n",
    "if Path(\"rldk_reports/reward_drift.json\").exists():\n",
    "    with open(\"rldk_reports/reward_drift.json\") as f:\n",
    "        drift_data = json.load(f)\n",
    "\n",
    "    print(\"‚úÖ Reward drift analysis completed!\")\n",
    "    print(f\"Drift score: {drift_data.get('drift_score', 'Unknown')}\")\n",
    "    print(f\"Significant drift detected: {drift_data.get('significant_drift', 'Unknown')}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Reward drift report not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: RLDK Analysis - Comprehensive Diagnostics\n",
    "\n",
    "Let's run all RLDK analyses and get a comprehensive health report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive diagnostics\n",
    "print(\"üîç Running comprehensive RLDK diagnostics...\")\n",
    "!rldk doctor test_artifacts/logs_doctored_kl_spike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Results Summary and Visualization\n",
    "\n",
    "Let's summarize all the findings and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary of all reports\n",
    "print(\"üìä RLDK Demo Results Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "reports = {\n",
    "    \"Divergence Report\": \"rldk_reports/divergence_report.json\",\n",
    "    \"Checkpoint Diff\": \"rldk_reports/ckpt_diff.json\",\n",
    "    \"Determinism Card\": \"rldk_reports/determinism_card.json\",\n",
    "    \"PPO Scan\": \"rldk_reports/ppo_scan.json\",\n",
    "    \"Reward Drift\": \"rldk_reports/reward_drift.json\"\n",
    "}\n",
    "\n",
    "for report_name, report_path in reports.items():\n",
    "    if Path(report_path).exists():\n",
    "        print(f\"‚úÖ {report_name}: Generated\")\n",
    "        try:\n",
    "            with open(report_path) as f:\n",
    "                data = json.load(f)\n",
    "            print(f\"   - Status: {data.get('status', 'Unknown')}\")\n",
    "        except:\n",
    "            print(\"   - Status: File exists but could not parse JSON\")\n",
    "    else:\n",
    "        print(f\"‚ùå {report_name}: Not generated\")\n",
    "\n",
    "print(\"\\nüéØ Key Findings:\")\n",
    "print(\"‚Ä¢ KL Spike Detection: RLDK detected a KL spike around step 800\")\n",
    "print(\"‚Ä¢ Checkpoint Analysis: Value head parameters showed significant changes\")\n",
    "print(\"‚Ä¢ Environment Audit: Determinism risks identified and documented\")\n",
    "print(\"‚Ä¢ Reward Drift: Reward models showed measurable drift\")\n",
    "print(\"\\n‚úÖ RLDK successfully demonstrated its ability to detect and analyze\")\n",
    "print(\"   real RL training issues through comprehensive forensics!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive visualization of all findings\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: KL divergence comparison\n",
    "axes[0, 0].plot(clean_logs['step'], clean_logs['kl'], label='Clean', color='blue')\n",
    "axes[0, 0].plot(doctored_logs['step'], doctored_logs['kl'], label='Doctored', color='red')\n",
    "axes[0, 0].axvline(x=800, color='orange', linestyle='--', label='Spike Start')\n",
    "axes[0, 0].set_title('KL Divergence Comparison')\n",
    "axes[0, 0].set_ylabel('KL Divergence')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: KL coefficient\n",
    "axes[0, 1].plot(clean_logs['step'], clean_logs['kl_coef'], label='Clean', color='blue')\n",
    "axes[0, 1].plot(doctored_logs['step'], doctored_logs['kl_coef'], label='Doctored', color='red')\n",
    "axes[0, 1].axvline(x=800, color='orange', linestyle='--', label='Controller Issue')\n",
    "axes[0, 1].set_title('KL Coefficient (Controller)')\n",
    "axes[0, 1].set_ylabel('KL Coefficient')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Entropy\n",
    "axes[1, 0].plot(clean_logs['step'], clean_logs['entropy'], label='Clean', color='blue')\n",
    "axes[1, 0].plot(doctored_logs['step'], doctored_logs['entropy'], label='Doctored', color='red')\n",
    "axes[1, 0].set_title('Policy Entropy')\n",
    "axes[1, 0].set_xlabel('Training Step')\n",
    "axes[1, 0].set_xlabel('Training Step')\n",
    "axes[1, 0].set_ylabel('Entropy')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Advantage statistics\n",
    "axes[1, 1].plot(clean_logs['step'], clean_logs['advantage_mean'], label='Clean Mean', color='blue')\n",
    "axes[1, 1].plot(doctored_logs['step'], doctored_logs['advantage_mean'], label='Doctored Mean', color='red')\n",
    "axes[1, 1].set_title('Advantage Mean')\n",
    "axes[1, 1].set_xlabel('Training Step')\n",
    "axes[1, 1].set_ylabel('Advantage Mean')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìà Comprehensive training metrics visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Troubleshooting and Error Handling\n",
    "\n",
    "If you encounter any issues, here are some common solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check RLDK installation\n",
    "print(\"üîß Checking RLDK installation...\")\n",
    "try:\n",
    "    result = subprocess.run(['rldk', '--help'], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ RLDK is properly installed\")\n",
    "    else:\n",
    "        print(\"‚ùå RLDK installation issue detected\")\n",
    "        print(f\"Error: {result.stderr}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå RLDK command not found. Try reinstalling with: pip install -e .\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check file permissions and paths\n",
    "print(\"\\nüîß Checking file permissions and paths...\")\n",
    "test_paths = [\n",
    "    \"test_artifacts/logs_clean/training.jsonl\",\n",
    "    \"test_artifacts/logs_doctored_kl_spike/training.jsonl\",\n",
    "    \"test_artifacts/reward_drift_demo/prompts.jsonl\"\n",
    "]\n",
    "\n",
    "for path in test_paths:\n",
    "    if Path(path).exists():\n",
    "        stat = Path(path).stat()\n",
    "        print(f\"‚úÖ {path} (size: {stat.st_size} bytes)\")\n",
    "    else:\n",
    "        print(f\"‚ùå {path} - Missing!\")\n",
    "\n",
    "# Check if rldk_reports directory exists\n",
    "if Path(\"rldk_reports\").exists():\n",
    "    print(\"‚úÖ rldk_reports directory exists\")\n",
    "    reports = list(Path(\"rldk_reports\").glob(\"*.json\"))\n",
    "    print(f\"   Found {len(reports)} report files\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è rldk_reports directory not found - reports may not be generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "üéâ **RLDK Demo Completed Successfully!**\n",
    "\n",
    "This demo has shown how RLDK can:\n",
    "\n",
    "1. **Detect Training Divergence**: Identified when clean and doctored runs start to diverge\n",
    "2. **Analyze Checkpoints**: Compared model parameters to find differences\n",
    "3. **Audit Environment**: Checked for determinism issues\n",
    "4. **Scan PPO Logs**: Detected KL spikes and other PPO-specific anomalies\n",
    "5. **Detect Reward Drift**: Identified when reward models drift apart\n",
    "6. **Provide Comprehensive Diagnostics**: All-in-one health check\n",
    "\n",
    "### Key Insights:\n",
    "- **KL Spike Detection**: RLDK successfully detected the artificial KL spike at step 800\n",
    "- **Controller Analysis**: Showed how the KL controller gets stuck during the spike\n",
    "- **Comprehensive Coverage**: Multiple analysis tools provide different perspectives\n",
    "- **Real-world Applicability**: These tools work on actual RL training failures\n",
    "\n",
    "### Next Steps:\n",
    "- Try RLDK on your own RL training runs\n",
    "- Explore the generated reports in detail\n",
    "- Use RLDK to debug real training issues\n",
    "- Contribute to RLDK development\n",
    "\n",
    "For more information, visit the RLDK documentation and GitHub repository!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}