[
  {
    "type": "high_reward_variance",
    "message": "Reward std 0.2000 exceeds threshold 0.05",
    "step": 1,
    "timestamp": 1757954577.1398988,
    "severity": "warning",
    "ppo_metrics": {
      "rollout_reward_mean": 0.5,
      "rollout_reward_std": 0.2,
      "rollout_reward_min": 0.0,
      "rollout_reward_max": 0.0,
      "rollout_length_mean": 0.0,
      "rollout_length_std": 0.0,
      "policy_kl_mean": 0.05,
      "policy_kl_std": 0.0,
      "policy_entropy_mean": 2.0,
      "policy_entropy_std": 0.0,
      "policy_clip_frac": 0.1,
      "policy_loss": 0.0,
      "value_loss": 0.3,
      "value_mean": 0.0,
      "value_std": 0.0,
      "advantage_mean": 0.0,
      "advantage_std": 0.0,
      "learning_rate": 1e-05,
      "gradient_norm": 0.5,
      "clip_ratio": 0.0,
      "tokens_per_second": 0.0,
      "samples_per_second": 0.0,
      "gpu_utilization": 0.0,
      "policy_collapse_risk": 0.0,
      "reward_hacking_risk": 0.0,
      "training_stability": 1.0
    }
  },
  {
    "type": "high_reward_variance",
    "message": "Reward std 0.2000 exceeds threshold 0.05",
    "step": 2,
    "timestamp": 1757954589.1910994,
    "severity": "warning",
    "ppo_metrics": {
      "rollout_reward_mean": 0.6,
      "rollout_reward_std": 0.2,
      "rollout_reward_min": 0.0,
      "rollout_reward_max": 0.0,
      "rollout_length_mean": 0.0,
      "rollout_length_std": 0.0,
      "policy_kl_mean": 0.060000000000000005,
      "policy_kl_std": 0.0,
      "policy_entropy_mean": 1.9,
      "policy_entropy_std": 0.0,
      "policy_clip_frac": 0.1,
      "policy_loss": 0.0,
      "value_loss": 0.25,
      "value_mean": 0.0,
      "value_std": 0.0,
      "advantage_mean": 0.0,
      "advantage_std": 0.0,
      "learning_rate": 1e-05,
      "gradient_norm": 0.5,
      "clip_ratio": 0.0,
      "tokens_per_second": 0.0,
      "samples_per_second": 0.0,
      "gpu_utilization": 0.0,
      "policy_collapse_risk": 0.0,
      "reward_hacking_risk": 0.0,
      "training_stability": 1.0
    }
  },
  {
    "type": "high_reward_variance",
    "message": "Reward std 0.2000 exceeds threshold 0.05",
    "step": 3,
    "timestamp": 1757954589.1920958,
    "severity": "warning",
    "ppo_metrics": {
      "rollout_reward_mean": 0.7,
      "rollout_reward_std": 0.2,
      "rollout_reward_min": 0.0,
      "rollout_reward_max": 0.0,
      "rollout_length_mean": 0.0,
      "rollout_length_std": 0.0,
      "policy_kl_mean": 0.07,
      "policy_kl_std": 0.0,
      "policy_entropy_mean": 1.8,
      "policy_entropy_std": 0.0,
      "policy_clip_frac": 0.1,
      "policy_loss": 0.0,
      "value_loss": 0.19999999999999998,
      "value_mean": 0.0,
      "value_std": 0.0,
      "advantage_mean": 0.0,
      "advantage_std": 0.0,
      "learning_rate": 1e-05,
      "gradient_norm": 0.5,
      "clip_ratio": 0.0,
      "tokens_per_second": 0.0,
      "samples_per_second": 0.0,
      "gpu_utilization": 0.0,
      "policy_collapse_risk": 0.0,
      "reward_hacking_risk": 0.0,
      "training_stability": 1.0
    }
  },
  {
    "type": "high_reward_variance",
    "message": "Reward std 0.2000 exceeds threshold 0.05",
    "step": 4,
    "timestamp": 1757954599.1864195,
    "severity": "warning",
    "ppo_metrics": {
      "rollout_reward_mean": 0.8,
      "rollout_reward_std": 0.2,
      "rollout_reward_min": 0.0,
      "rollout_reward_max": 0.0,
      "rollout_length_mean": 0.0,
      "rollout_length_std": 0.0,
      "policy_kl_mean": 0.08,
      "policy_kl_std": 0.0,
      "policy_entropy_mean": 1.7,
      "policy_entropy_std": 0.0,
      "policy_clip_frac": 0.1,
      "policy_loss": 0.0,
      "value_loss": 0.14999999999999997,
      "value_mean": 0.0,
      "value_std": 0.0,
      "advantage_mean": 0.0,
      "advantage_std": 0.0,
      "learning_rate": 1e-05,
      "gradient_norm": 0.5,
      "clip_ratio": 0.0,
      "tokens_per_second": 0.0,
      "samples_per_second": 0.0,
      "gpu_utilization": 0.0,
      "policy_collapse_risk": 0.0,
      "reward_hacking_risk": 0.0,
      "training_stability": 1.0
    }
  },
  {
    "type": "high_reward_variance",
    "message": "Reward std 0.2000 exceeds threshold 0.05",
    "step": 5,
    "timestamp": 1757954599.187495,
    "severity": "warning",
    "ppo_metrics": {
      "rollout_reward_mean": 0.9,
      "rollout_reward_std": 0.2,
      "rollout_reward_min": 0.0,
      "rollout_reward_max": 0.0,
      "rollout_length_mean": 0.0,
      "rollout_length_std": 0.0,
      "policy_kl_mean": 0.09,
      "policy_kl_std": 0.0,
      "policy_entropy_mean": 1.6,
      "policy_entropy_std": 0.0,
      "policy_clip_frac": 0.1,
      "policy_loss": 0.0,
      "value_loss": 0.09999999999999998,
      "value_mean": 0.0,
      "value_std": 0.0,
      "advantage_mean": 0.0,
      "advantage_std": 0.0,
      "learning_rate": 1e-05,
      "gradient_norm": 0.5,
      "clip_ratio": 0.0,
      "tokens_per_second": 0.0,
      "samples_per_second": 0.0,
      "gpu_utilization": 0.0,
      "policy_collapse_risk": 0.0,
      "reward_hacking_risk": 0.0,
      "training_stability": 1.0
    }
  }
]